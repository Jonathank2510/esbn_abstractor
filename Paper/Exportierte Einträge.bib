
@misc{altabaa_abstractors_2023,
	title = {Abstractors and relational cross-attention: {An} inductive bias for explicit relational reasoning in {Transformers}},
	shorttitle = {Abstractors and relational cross-attention},
	url = {http://arxiv.org/abs/2304.00195},
	doi = {10.48550/arXiv.2304.00195},
	abstract = {An extension of Transformers is proposed that enables explicit relational reasoning through a novel module called the Abstractor. At the core of the Abstractor is a variant of attention called relational cross-attention. The approach is motivated by an architectural inductive bias for relational learning that disentangles relational information from extraneous features about individual objects. This enables explicit relational reasoning, supporting abstraction and generalization from limited data. The Abstractor is first evaluated on simple discriminative relational tasks and compared to existing relational architectures. Next, the Abstractor is evaluated on purely relational sequence-to-sequence tasks, where dramatic improvements are seen in sample efficiency compared to standard Transformers. Finally, Abstractors are evaluated on a collection of tasks based on mathematical problem solving, where modest but consistent improvements in performance and sample efficiency are observed.},
	urldate = {2024-03-08},
	publisher = {arXiv},
	author = {Altabaa, Awni and Webb, Taylor and Cohen, Jonathan and Lafferty, John},
	month = oct,
	year = {2023},
	note = {arXiv:2304.00195 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/jonathankonig/Zotero/storage/YR6KTEBX/Altabaa et al. - 2023 - Abstractors and relational cross-attention An ind.pdf:application/pdf;arXiv.org Snapshot:/Users/jonathankonig/Zotero/storage/XCCTHSXH/2304.html:text/html},
}

@article{webb_relational_2023,
	title = {The {Relational} {Bottleneck} as an {Inductive} {Bias} for {Efficient} {Abstraction}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2309.06629},
	doi = {10.48550/ARXIV.2309.06629},
	abstract = {A central challenge for cognitive science is to explain how abstract concepts are acquired from limited experience. This effort has often been framed in terms of a dichotomy between connectionist and symbolic cognitive models. Here, we highlight a recently emerging line of work that suggests a novel reconciliation of these approaches, by exploiting an inductive bias that we term the relational bottleneck. We review a family of models that employ this approach to induce abstractions in a data-efficient manner, emphasizing their potential as candidate models for the acquisition of abstract concepts in the human mind and brain.},
	urldate = {2024-03-02},
	author = {Webb, Taylor W. and Frankland, Steven M. and Altabaa, Awni and Krishnamurthy, Kamesh and Campbell, Declan and Russin, Jacob and O'Reilly, Randall and Lafferty, John and Cohen, Jonathan D.},
	year = {2023},
	file = {Webb et al. - 2023 - The Relational Bottleneck as an Inductive Bias for.pdf:/Users/jonathankonig/Zotero/storage/SZBPYE9C/Webb et al. - 2023 - The Relational Bottleneck as an Inductive Bias for.pdf:application/pdf},
}

@misc{webb_emergent_2021,
	title = {Emergent {Symbols} through {Binding} in {External} {Memory}},
	url = {http://arxiv.org/abs/2012.14601},
	abstract = {A key aspect of human intelligence is the ability to infer abstract rules directly from high-dimensional sensory data, and to do so given only a limited amount of training experience. Deep neural network algorithms have proven to be a powerful tool for learning directly from high-dimensional data, but currently lack this capacity for data-efficient induction of abstract rules, leading some to argue that symbol-processing mechanisms will be necessary to account for this capacity. In this work, we take a step toward bridging this gap by introducing the Emergent Symbol Binding Network (ESBN), a recurrent network augmented with an external memory that enables a form of variable-binding and indirection. This binding mechanism allows symbol-like representations to emerge through the learning process without the need to explicitly incorporate symbol-processing machinery, enabling the ESBN to learn rules in a manner that is abstracted away from the particular entities to which those rules apply. Across a series of tasks, we show that this architecture displays nearly perfect generalization of learned rules to novel entities given only a limited number of training examples, and outperforms a number of other competitive neural network architectures.},
	urldate = {2024-03-02},
	author = {Webb, Taylor W. and Sinha, Ishan and Cohen, Jonathan D.},
	month = mar,
	year = {2021},
	note = {arXiv:2012.14601 [cs]},
	file = {Webb et al. - 2021 - Emergent Symbols through Binding in External Memory.pdf:/Users/jonathankonig/Zotero/storage/KJUJD3RM/Webb et al. - 2021 - Emergent Symbols through Binding in External Memory.pdf:application/pdf},
}

@misc{webb_learning_2023,
	title = {Learning {Representations} that {Support} {Extrapolation}},
	url = {http://arxiv.org/abs/2007.05059},
	doi = {10.48550/arXiv.2007.05059},
	abstract = {Extrapolation -- the ability to make inferences that go beyond the scope of one's experiences -- is a hallmark of human intelligence. By contrast, the generalization exhibited by contemporary neural network algorithms is largely limited to interpolation between data points in their training corpora. In this paper, we consider the challenge of learning representations that support extrapolation. We introduce a novel visual analogy benchmark that allows the graded evaluation of extrapolation as a function of distance from the convex domain defined by the training data. We also introduce a simple technique, temporal context normalization, that encourages representations that emphasize the relations between objects. We find that this technique enables a significant improvement in the ability to extrapolate, considerably outperforming a number of competitive techniques.},
	urldate = {2024-03-08},
	publisher = {arXiv},
	author = {Webb, Taylor W. and Dulberg, Zachary and Frankland, Steven M. and Petrov, Alexander A. and O'Reilly, Randall C. and Cohen, Jonathan D.},
	month = sep,
	year = {2023},
	note = {arXiv:2007.05059 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/jonathankonig/Zotero/storage/WCKFLPIH/Webb et al. - 2023 - Learning Representations that Support Extrapolatio.pdf:application/pdf;arXiv.org Snapshot:/Users/jonathankonig/Zotero/storage/UVZ3H225/2007.html:text/html},
}

@misc{russin_compositional_2019,
	title = {Compositional generalization in a deep seq2seq model by separating syntax and semantics},
	url = {http://arxiv.org/abs/1904.09708},
	doi = {10.48550/arXiv.1904.09708},
	abstract = {Standard methods in deep learning for natural language processing fail to capture the compositional structure of human language that allows for systematic generalization outside of the training distribution. However, human learners readily generalize in this way, e.g. by applying known grammatical rules to novel words. Inspired by work in neuroscience suggesting separate brain systems for syntactic and semantic processing, we implement a modification to standard approaches in neural machine translation, imposing an analogous separation. The novel model, which we call Syntactic Attention, substantially outperforms standard methods in deep learning on the SCAN dataset, a compositional generalization task, without any hand-engineered features or additional supervision. Our work suggests that separating syntactic from semantic learning may be a useful heuristic for capturing compositional structure.},
	urldate = {2024-03-08},
	publisher = {arXiv},
	author = {Russin, Jake and Jo, Jason and O'Reilly, Randall C. and Bengio, Yoshua},
	month = may,
	year = {2019},
	note = {arXiv:1904.09708 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/jonathankonig/Zotero/storage/7W7VLUXS/Russin et al. - 2019 - Compositional generalization in a deep seq2seq mod.pdf:application/pdf;arXiv.org Snapshot:/Users/jonathankonig/Zotero/storage/W3RRXP2K/1904.html:text/html},
}

@misc{lake_generalization_2018,
	title = {Generalization without systematicity: {On} the compositional skills of sequence-to-sequence recurrent networks},
	shorttitle = {Generalization without systematicity},
	url = {http://arxiv.org/abs/1711.00350},
	abstract = {Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb "dax," he or she can immediately understand the meaning of "dax twice" or "sing and dax." In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generalizations when the differences between training and test commands are small, so that they can apply "mix-and-match" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the "dax" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks' notorious training data thirst.},
	urldate = {2024-03-14},
	publisher = {arXiv},
	author = {Lake, Brenden M. and Baroni, Marco},
	month = jun,
	year = {2018},
	note = {arXiv:1711.00350 [cs]
version: 3},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/jonathankonig/Zotero/storage/Z6GWB3D2/1711.html:text/html;Full Text PDF:/Users/jonathankonig/Zotero/storage/N9ARK7WT/Lake und Baroni - 2018 - Generalization without systematicity On the compo.pdf:application/pdf},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2024-04-03},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/jonathankonig/Zotero/storage/IGMT5GMN/Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/jonathankonig/Zotero/storage/Z6KMQI5A/1706.html:text/html},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/9/8/1735-1780/6109},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	language = {en},
	number = {8},
	urldate = {2024-04-04},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
}

@misc{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2024-04-09},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/jonathankonig/Zotero/storage/EJAGUPTX/Kingma und Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/Users/jonathankonig/Zotero/storage/YI53G948/1412.html:text/html},
}
