{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathankonig/miniforge3/envs/esbn_abstractor/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/Users/jonathankonig/miniforge3/envs/esbn_abstractor/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from Models.Transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseError",
     "evalue": "Failed to construct dataset scan: Message type \"tensorflow_datasets.DatasetInfo\" has no field named \"fileFormat\" at \"DatasetInfo\".\n Available Fields(except extensions): \"['name', 'description', 'version', 'configName', 'configDescription', 'citation', 'sizeInBytes', 'downloadSize', 'location', 'downloadChecksums', 'schema', 'splits', 'supervisedKeys', 'redistributionInfo', 'moduleName', 'disableShuffling']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/esbn_abstractor/lib/python3.9/site-packages/google/protobuf/json_format.py:544\u001b[0m, in \u001b[0;36m_Parser._ConvertFieldValuePair\u001b[0;34m(self, js, message, path)\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ParseError(\n\u001b[1;32m    545\u001b[0m       (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMessage type \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m has no field named \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m at \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    546\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Available Fields(except extensions): \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    547\u001b[0m            message_descriptor\u001b[38;5;241m.\u001b[39mfull_name, name, path,\n\u001b[1;32m    548\u001b[0m            [f\u001b[38;5;241m.\u001b[39mjson_name \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m message_descriptor\u001b[38;5;241m.\u001b[39mfields]))\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m names:\n",
      "\u001b[0;31mParseError\u001b[0m: Message type \"tensorflow_datasets.DatasetInfo\" has no field named \"fileFormat\" at \"DatasetInfo\".\n Available Fields(except extensions): \"['name', 'description', 'version', 'configName', 'configDescription', 'citation', 'sizeInBytes', 'downloadSize', 'location', 'downloadChecksums', 'schema', 'splits', 'supervisedKeys', 'redistributionInfo', 'moduleName', 'disableShuffling']\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mParseError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load training data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_raw, test_raw \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscan/addprim_jump\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/esbn_abstractor/lib/python3.9/site-packages/tensorflow_datasets/core/load.py:330\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m builder_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    328\u001b[0m   builder_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 330\u001b[0m dbuilder \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_gcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_gcs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[1;32m    332\u001b[0m   download_and_prepare_kwargs \u001b[38;5;241m=\u001b[39m download_and_prepare_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n",
      "File \u001b[0;32m~/miniforge3/envs/esbn_abstractor/lib/python3.9/site-packages/tensorflow_datasets/core/load.py:177\u001b[0m, in \u001b[0;36mbuilder\u001b[0;34m(name, try_gcs, **builder_kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m:\n\u001b[1;32m    176\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m py_utils\u001b[38;5;241m.\u001b[39mtry_reraise(prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to construct dataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pytype: disable=not-instantiable\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# If neither the code nor the files are found, raise DatasetNotFoundError\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m not_found_error\n",
      "File \u001b[0;32m~/miniforge3/envs/esbn_abstractor/lib/python3.9/site-packages/tensorflow_datasets/core/dataset_builder.py:923\u001b[0m, in \u001b[0;36mFileReaderBuilder.__init__\u001b[0;34m(self, file_format, **kwargs)\u001b[0m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    912\u001b[0m         file_adapters\u001b[38;5;241m.\u001b[39mFileFormat] \u001b[38;5;241m=\u001b[39m file_adapters\u001b[38;5;241m.\u001b[39mDEFAULT_FILE_FORMAT,\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[1;32m    914\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Initializes an instance of FileReaderBuilder.\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \n\u001b[1;32m    916\u001b[0m \u001b[38;5;124;03m  Callers must pass arguments as keyword arguments.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;124;03m    **kwargs: Arguments passed to `DatasetBuilder`.\u001b[39;00m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 923\u001b[0m   \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    924\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file_format \u001b[38;5;241m=\u001b[39m file_adapters\u001b[38;5;241m.\u001b[39mFileFormat(file_format)\n",
      "File \u001b[0;32m~/miniforge3/envs/esbn_abstractor/lib/python3.9/site-packages/tensorflow_datasets/core/dataset_builder.py:182\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[0;34m(self, data_dir, config, version)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_dir_root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_data_dir(data_dir)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_dir):\n\u001b[0;32m--> 182\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_from_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Use the code version (do not restore data)\u001b[39;00m\n\u001b[1;32m    184\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39minitialize_from_bucket()\n",
      "File \u001b[0;32m~/miniforge3/envs/esbn_abstractor/lib/python3.9/site-packages/tensorflow_datasets/core/dataset_info.py:377\u001b[0m, in \u001b[0;36mDatasetInfo.read_from_directory\u001b[0;34m(self, dataset_info_dir)\u001b[0m\n\u001b[1;32m    369\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry to load `DatasetInfo` from a directory which does not exist or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not contain `dataset_info.json`. Please delete the directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_info_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`  if you are trying to re-generate the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    373\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    374\u001b[0m   )\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# Load the metadata from disk\u001b[39;00m\n\u001b[0;32m--> 377\u001b[0m parsed_proto \u001b[38;5;241m=\u001b[39m \u001b[43mread_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# Update splits\u001b[39;00m\n\u001b[1;32m    380\u001b[0m split_dict \u001b[38;5;241m=\u001b[39m splits_lib\u001b[38;5;241m.\u001b[39mSplitDict\u001b[38;5;241m.\u001b[39mfrom_proto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, parsed_proto\u001b[38;5;241m.\u001b[39msplits)\n",
      "File \u001b[0;32m~/miniforge3/envs/esbn_abstractor/lib/python3.9/site-packages/tensorflow_datasets/core/dataset_info.py:558\u001b[0m, in \u001b[0;36mread_from_json\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    556\u001b[0m json_str \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mas_path(path)\u001b[38;5;241m.\u001b[39mread_text()\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# Parse it back into a proto.\u001b[39;00m\n\u001b[0;32m--> 558\u001b[0m parsed_proto \u001b[38;5;241m=\u001b[39m \u001b[43mjson_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_info_pb2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDatasetInfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parsed_proto\n",
      "File \u001b[0;32m~/miniforge3/envs/esbn_abstractor/lib/python3.9/site-packages/google/protobuf/json_format.py:436\u001b[0m, in \u001b[0;36mParse\u001b[0;34m(text, message, ignore_unknown_fields, descriptor_pool, max_recursion_depth)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    435\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ParseError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to load JSON: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mstr\u001b[39m(e)))\n\u001b[0;32m--> 436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParseDict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_unknown_fields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescriptor_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmax_recursion_depth\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/esbn_abstractor/lib/python3.9/site-packages/google/protobuf/json_format.py:461\u001b[0m, in \u001b[0;36mParseDict\u001b[0;34m(js_dict, message, ignore_unknown_fields, descriptor_pool, max_recursion_depth)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Parses a JSON dictionary representation into a message.\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \n\u001b[1;32m    447\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;124;03m  The same message passed as argument.\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    460\u001b[0m parser \u001b[38;5;241m=\u001b[39m _Parser(ignore_unknown_fields, descriptor_pool, max_recursion_depth)\n\u001b[0;32m--> 461\u001b[0m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConvertMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjs_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m message\n",
      "File \u001b[0;32m~/miniforge3/envs/esbn_abstractor/lib/python3.9/site-packages/google/protobuf/json_format.py:502\u001b[0m, in \u001b[0;36m_Parser.ConvertMessage\u001b[0;34m(self, value, message, path)\u001b[0m\n\u001b[1;32m    500\u001b[0m   methodcaller(_WKTJSONMETHODS[full_name][\u001b[38;5;241m1\u001b[39m], value, message, path)(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 502\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ConvertFieldValuePair\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecursion_depth \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/esbn_abstractor/lib/python3.9/site-packages/google/protobuf/json_format.py:629\u001b[0m, in \u001b[0;36m_Parser._ConvertFieldValuePair\u001b[0;34m(self, js, message, path)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ParseError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to parse \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m field: \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, e))\n\u001b[1;32m    628\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 629\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ParseError(\u001b[38;5;28mstr\u001b[39m(e))\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    631\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ParseError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to parse \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m field: \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, e))\n",
      "\u001b[0;31mParseError\u001b[0m: Failed to construct dataset scan: Message type \"tensorflow_datasets.DatasetInfo\" has no field named \"fileFormat\" at \"DatasetInfo\".\n Available Fields(except extensions): \"['name', 'description', 'version', 'configName', 'configDescription', 'citation', 'sizeInBytes', 'downloadSize', 'location', 'downloadChecksums', 'schema', 'splits', 'supervisedKeys', 'redistributionInfo', 'moduleName', 'disableShuffling']\""
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "train_raw, test_raw = tfds.load(\"scan/addprim_jump\", split=[\"train\", \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'walk after look right', shape=(), dtype=string)\n",
      "tf.Tensor(b'walk left thrice after walk left thrice', shape=(), dtype=string)\n",
      "tf.Tensor(b'look right twice and turn around left twice', shape=(), dtype=string)\n",
      "tf.Tensor(b'jump', shape=(), dtype=string)\n",
      "tf.Tensor(b'turn around right thrice after run right', shape=(), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-15 09:17:04.460491: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_2}}]]\n",
      "2024-03-15 09:17:04.461647: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2024-03-15 09:17:04.512309: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "train_raw.shuffle(train_raw.length).batch(32).prefetch(32)\n",
    "def standardize(text):\n",
    "    text = tf.string.join([\"<SOS>\", text, \"<EOS>\"], separator=' ')\n",
    "    return text\n",
    "\n",
    "\n",
    "command_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=standardize\n",
    ")\n",
    "action_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=standardize\n",
    ")\n",
    "\n",
    "command_processor.adapt(train_raw.map(lambda input: input[\"command\"]))\n",
    "action_processor.adapt(train_raw.map(lambda input: input[\"action\"]))\n",
    "\n",
    "def process_scan(command, action):\n",
    "    command = command_processor(command).to_tensor()\n",
    "    action = action_processor(action)\n",
    "    action_in = action[:,:-1].to_tensor()\n",
    "    action_out = action[:,1:].to_tensor()\n",
    "    return (command, action_in), action_out\n",
    "\n",
    "train_ds = train_raw.map(process_scan, tf.data.AUTOTUNE)\n",
    "val_ds = test_raw.map(process_scan, tf.data.AUTOTUNE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
